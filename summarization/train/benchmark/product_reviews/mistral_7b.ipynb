{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import torch \n",
    "import pandas as pd\n",
    "from torchmetrics.text import ROUGEScore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = ROUGEScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.0, top_p=0.95, max_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_df = pd.read_csv(\"../../../data/labelled/reviews/splits/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-14 04:27:13 config.py:748] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 04-14 04:27:13 llm_engine.py:75] Initializing an LLM engine (v0.4.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', tokenizer='mistralai/Mistral-7B-Instruct-v0.1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "INFO 04-14 04:27:13 selector.py:45] Cannot use FlashAttention because the package is not found. Please install it for better performance.\n",
      "INFO 04-14 04:27:13 selector.py:21] Using XFormers backend.\n",
      "INFO 04-14 04:27:15 weight_utils.py:177] Using model weights format ['*.safetensors']\n",
      "INFO 04-14 04:27:21 model_runner.py:104] Loading model weights took 13.4966 GB\n",
      "INFO 04-14 04:27:23 gpu_executor.py:94] # GPU blocks: 29400, # CPU blocks: 2048\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"mistralai/Mistral-7B-Instruct-v0.1\", dtype=torch.float16, gpu_memory_utilization=0.95, enforce_eager=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for idx in range(0, len(rev_df), batch_size):\n",
    "    if (idx)/(batch_size*100) == 0: print(f\"processed {idx+1} rows\")\n",
    "    batch = rev_df[idx:idx+batch_size]\n",
    "    descs = batch.input_reviews.to_list()\n",
    "    labels = batch.label_reviews.to_list()\n",
    "    input_batch = [\"<s>[INST] \"+desc+\" [/INST]\" for desc in descs]\n",
    "    outputs = llm.generate(input_batch, sampling_params, use_tqdm=False)\n",
    "    preds.extend([out.outputs[0].text.strip() for out in outputs])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_df['mistral'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_df.to_csv(\"mistral_results_reviews.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_reviews</th>\n",
       "      <th>label_reviews</th>\n",
       "      <th>mistral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Below are the reviews of a product: \\n- I purc...</td>\n",
       "      <td>pros:\\n- Reduces glare\\n- Very impressive\\n- W...</td>\n",
       "      <td>Pros:\\n\\n* High-quality multi-coated glass\\n* ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Below are the reviews of a product: \\n- Well w...</td>\n",
       "      <td>pros:\\n- Huge capacity\\n- Beautiful design\\n- ...</td>\n",
       "      <td>The Astro Pro 20000mAh 4-Port Aluminum Portabl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       input_reviews  \\\n",
       "0  Below are the reviews of a product: \\n- I purc...   \n",
       "1  Below are the reviews of a product: \\n- Well w...   \n",
       "\n",
       "                                       label_reviews  \\\n",
       "0  pros:\\n- Reduces glare\\n- Very impressive\\n- W...   \n",
       "1  pros:\\n- Huge capacity\\n- Beautiful design\\n- ...   \n",
       "\n",
       "                                             mistral  \n",
       "0  Pros:\\n\\n* High-quality multi-coated glass\\n* ...  \n",
       "1  The Astro Pro 20000mAh 4-Port Aluminum Portabl...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rev_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_scores = {\"rouge1\":[], \"rouge2\":[], \"rougeL\": []}\n",
    "for pred, target in zip(rev_df.label_reviews.to_list(), rev_df.mistral.to_list()):\n",
    "    rouge_all = rouge(pred, target)\n",
    "    rouge_scores['rouge1'].append(rouge_all['rouge1_fmeasure'].item())\n",
    "    rouge_scores['rouge2'].append(rouge_all['rouge2_fmeasure'].item())\n",
    "    rouge_scores['rougeL'].append(rouge_all['rougeL_fmeasure'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_df['rouge1'] = rouge_scores['rouge1']\n",
    "rev_df['rouge2'] = rouge_scores['rouge2']\n",
    "rev_df['rougeL'] = rouge_scores['rougeL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_df.to_csv(\"mistral_results_reviews.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {\"rouge1\": rev_df.rouge1.mean(), \"rouge2\": rev_df.rouge2.mean(), \"rougeL\": rev_df.rougeL.mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.3995208429379389,\n",
       " 'rouge2': 0.157890436598612,\n",
       " 'rougeL': 0.27184627608354206}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
