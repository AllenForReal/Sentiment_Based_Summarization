{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import torch \n",
    "import pandas as pd\n",
    "from torchmetrics.text import ROUGEScore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = ROUGEScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.0, top_p=0.95, max_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_df = pd.read_csv(\"../../../data/labelled/reviews/splits/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-14 11:40:40 llm_engine.py:75] Initializing an LLM engine (v0.4.0) with config: model='llama_model', tokenizer='llama_model', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-14 11:40:40 selector.py:45] Cannot use FlashAttention because the package is not found. Please install it for better performance.\n",
      "INFO 04-14 11:40:40 selector.py:21] Using XFormers backend.\n",
      "INFO 04-14 11:40:45 model_runner.py:104] Loading model weights took 12.5523 GB\n",
      "INFO 04-14 11:40:46 gpu_executor.py:94] # GPU blocks: 7934, # CPU blocks: 512\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"llama_model\", dtype=torch.float16, gpu_memory_utilization=0.95, enforce_eager=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for idx in range(0, len(rev_df), batch_size):\n",
    "    batch = rev_df[idx:idx+batch_size]\n",
    "    descs = batch.input_reviews.to_list()\n",
    "    labels = batch.label_reviews.to_list()\n",
    "    input_batch = [\"<s>[INST] \"+desc+\" [/INST]\" for desc in descs]\n",
    "    outputs = llm.generate(input_batch, sampling_params, use_tqdm=False)\n",
    "    preds.extend([out.outputs[0].text.strip() for out in outputs])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_df['llama'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_df.to_csv(\"llama_results_reviews.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_reviews</th>\n",
       "      <th>label_reviews</th>\n",
       "      <th>llama</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Below are the reviews of a product: \\n- I purc...</td>\n",
       "      <td>pros:\\n- Reduces glare\\n- Very impressive\\n- W...</td>\n",
       "      <td>Based on the reviews provided, here are the pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Below are the reviews of a product: \\n- Well w...</td>\n",
       "      <td>pros:\\n- Huge capacity\\n- Beautiful design\\n- ...</td>\n",
       "      <td>Based on the reviews provided, here are the pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       input_reviews  \\\n",
       "0  Below are the reviews of a product: \\n- I purc...   \n",
       "1  Below are the reviews of a product: \\n- Well w...   \n",
       "\n",
       "                                       label_reviews  \\\n",
       "0  pros:\\n- Reduces glare\\n- Very impressive\\n- W...   \n",
       "1  pros:\\n- Huge capacity\\n- Beautiful design\\n- ...   \n",
       "\n",
       "                                               llama  \n",
       "0  Based on the reviews provided, here are the pr...  \n",
       "1  Based on the reviews provided, here are the pr...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rev_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_scores = {\"rouge1\":[], \"rouge2\":[], \"rougeL\": []}\n",
    "for pred, target in zip(rev_df.label_reviews.to_list(), rev_df.llama.to_list()):\n",
    "    if str(target).lower() == \"nan\":\n",
    "        target = \"\"\n",
    "    rouge_all = rouge(pred, target)\n",
    "    rouge_scores['rouge1'].append(rouge_all['rouge1_fmeasure'].item())\n",
    "    rouge_scores['rouge2'].append(rouge_all['rouge2_fmeasure'].item())\n",
    "    rouge_scores['rougeL'].append(rouge_all['rougeL_fmeasure'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_df['rouge1'] = rouge_scores['rouge1']\n",
    "rev_df['rouge2'] = rouge_scores['rouge2']\n",
    "rev_df['rougeL'] = rouge_scores['rougeL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_df.to_csv(\"llama_results_reviews.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {\"rouge1\": rev_df.rouge1.mean(), \"rouge2\": rev_df.rouge2.mean(), \"rougeL\": rev_df.rougeL.mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.3298645227672532,\n",
       " 'rouge2': 0.1131155245625414,\n",
       " 'rougeL': 0.20740494322949088}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
